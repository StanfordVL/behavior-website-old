<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <title>iGibson Challenge 2021</title>

  <!-- Bootstrap -->
  <link href="css/bootstrap.min.css" rel="stylesheet">
  <link href="css/animate.min.css" rel="stylesheet">
  <link href="css/animate.css" rel="stylesheet" />
  <link href="css/prettyPhoto.css" rel="stylesheet">
  <link href="css/style.css" rel="stylesheet">
  <!-- =======================================================
    Theme Name: OnePage
    Theme URL: https://bootstrapmade.com/onepage-multipurpose-bootstrap-template/
    Author: BootstrapMade
    Author URL: https://bootstrapmade.com
  ======================================================= -->

  <script src="https://kit.fontawesome.com/39b62c948b.js" crossorigin="anonymous"></script>

</head>
<style>
  .alignright {
    float: right;
  }
</style>

<body>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="row">
        <div class="site-logo2">
          <img height="45px" src="http://svl.stanford.edu//assets/images/c88c46b9e14bf897e57e597109c97005.png"> </img>
          <a class="brand">BEHAVIOR Challenge @ ICCV 2021</a>
          <div>
            <a class="brand">
              <h5 class="pull-left">Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments</h5>
              <h5 class="pull-right"> hosted by the Stanford Vision and Learning Lab</h5>
            </a>
          </div>
        </div>


        <!-- /.Navbar-collapse -->
      </div>
    </div>
  </nav>

  <section id="home">
  </section>
  <section id="about">
  </section>


  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
			 <h4> <a href="https://docs.google.com/document/d/1u2m9Ld6Qo3eG-fvCzuAZN6lHxwpVVBlwLVdzo_WDlNI/edit?usp=sharing">[Participation Guidelines]</a> <a href="https://eval.ai/web/challenges/challenge-page/1190/overview">[Registration]</a>  <a href="https://github.com/StanfordVL/BehaviorChallenge2021">[Starter Code]</a> <a href="activity_list.html">[100 Activities]</a> </h4>
            <p class="section-description">
              A primary goal of embodied AI research is to develop intelligent agents that can assist humans in their everyday lives in activities like washing dishes or cleaning floors. While recent years we have seen excellent new benchmarks that helped the field, they are usually restricted to a few tasks, short horizon or in simplified scenarios. Solving real-world challenges requires a new benchmark with realistic, diverse, and complex activities. We present BEHAVIOR, a benchmark with the 100 household activities that represent a new challenge for embodied AI solutions.
            </p>

            <p class="section-description">
              BEHAVIOR is a challenge <b>in simulation</b> where embodied agents make continuous full-body control decisions based on sensor information. Agents need to navigate and manipulate the simulated environment with the goal of acomplishing 100 household activities (see <a href="media/mosaic_behavior.jpg">here</a> all the activities in pairs of simulated and real-world images). BEHAVIOR tests the ability to perceive the environment, plan, and execute complex long-horizon activities that involve multiple objects, rooms, and state transitions, all with the reproducibility, safety and observability offered by a realistic physics simulation.
            </p>

            <p class="section-description">
              The results for this first edition of BEHAVIOR will be presented at ICCV21 in October 17th. Together with the announcement of the ranking, we will hold an exciting workshop on embodied AI interactive problems for long horizon activities, with an exciting set of world-renowed speakers from the field. Check our <a href="#workshop_speakers">list of speakers</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Characteristics of BEHAVIOR</h2>
            <div class="row">
              <div class="col-lg-4 col-md-12">
                <div class="box2 intro">
                  <h4 class="title"><b>100 Common Household Activities</b></h4>
                  <a href="media/mosaic_behavior.jpg">
                    <img src="media/beha.gif" width=210 height=210> </img>
                  </a>
                </div>
              </div>
              <div class="col-lg-4 col-md-12">
                <div class="box2 intro">
                  <h4 class="title"><b>Full Control based on Visual Input</b></h4>
                  <img src="media/tpv.jpg" width=210 height=210> </img>
                </div>
              </div>
              <div class="col-lg-4 col-md-12">
                <div class="box2 intro">
                  <h4 class="title"><b>Involving Multiple State Changes</b></h4>
                  <img src="media/sliced.gif" width=210 height=210> </img>
                </div>
              </div>
            </div>
            <hr>
          </div>
        </div>
      </div>
<!--       <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
           The activities in included in BEHAVIOR are:
            <br>
            <ul>
              <li>Realistic: Preserve high fidelity to the real in the activities, the models, the sensing, and actuation of the agent.</li>
              <li>Diverse: Present variation of scenes, objects, and types of activities.</li>
              <li>Complex: with large time length, number of objects, required skills, and state changes.</li>
            </ul>
            <br>
          </div>
        </div>
      </div> -->
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Setup</h2>
            <ul>
              <li><b>Embodiment:</b> We accept solutions using any of these two available embodiments:
              <ul>
                <li><b>BEHAVIOR Robot:</b> Humanoid avatar with two hands, a head and a torso.</li> 
                <li><b>Fetch Robot:</b> Model of the commercially available robot Fetch with a single arm.</li>
              </ul>
              </li>
              Both embodiments generate the same observations but differ on the actuation.
              <li><b>Observations:</b> Participants can choose between two observation modes with different observations available to the agent. Solutions using different observation modes are evaluated separately:
              <ul>
                <li><b>Observation Mode 1 - On-board Observations:</b> The agent only gets visual signals at 30 fps including RGB, depth, semantic segmentation, and segmentation of activity-relevant objects, and proprioceptive information. </li> 
                <li><b>Observation Mode 2 - Full observability:</b> In addition to the signals of the Mode 1, the agent has access to privileged information about the state of the environment such as the object poses and their state.</li>
              </ul>
              </li>
              <li><b>Actions:</b> Delta motion for each degree of freedom. For the BEHAVIOR Robot, the agent controls the motion of each hand (6 degrees of freedom) and the closing (1 degree of freedom), the head motion (6 degrees of freedome), and the torso motion (6 degrees of freedome). Head and hands are constrained to certain region around the torso. For the Fetch Robot, the agent controls the delta of each joint of the arm (7 degrees of freedom) and the closing (1 degree of freedom), the head (2 degrees of freedom) and the motion of the base (3 degrees of freedom).</li>
              <li><b>Action Primitives:</b>To facilitate the development of solutions, we provide a set of action primitives using motion planners for simple skills like picking an object or placing it in the environment. These primitives can bootstrap the creation of your own solution but are not sufficient to achieve all 100 BEHAVIOR activities. </li>
              <li><b>Grasping:</b> We provide two grasping modes, realistic and "sticky hand". Both are activated using one degree of freedom of the action space. Solutions using different grasping modes are evaluated separately:
              <ul>
                <li><b>Grasping Mode 1 - Realistic:</b> Objects need to be grasped by closing the hand around them, requiring more accurate pregrasping procedures.</li> 
                <li><b>Grasping Mode 2 - Sticky Hand:</b> Once the grasping is activated, any point of the hand/end-effector in contact with the object would be rigidly connected. This simplifies grasping.</li>
              </ul>
              <li><b>Reward:</b> We provide feedback about the completion of the task. Our tasks are defined by logic conditions and we provide reward for each component of the goal condition that turns TRUE. Feel free
                to create your own.</li>
              <li><b>Termination conditions:</b> The lenght of the episodes are variable depending on the activity and the scene, providing at least 3 times the length required for a human to acomplish it. We also terminate the episode if all elements of the goal condition become TRUE indicating success on the activity.</li>
            </ul>
            <p>The tech spec for the robots and the camera sensor can be found in our <a
                href="https://github.com/StanfordVL/BehaviorChallenge2021/blob/master/Parameters.md">starter code</a>.
            </p>
            <hr>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Evaluation, Metrics and Scoring</h2>
            We evaluate solutions based on the fraction of the elements in the goal condition that are turned TRUE. A solution is evaluated in all 100 activities, in three different types of instances: a) similar to training (only changing location of task relevant objects), b) with different object instances but in the same scenes as in training, and c) in new scenes not seen during training. We use three instances of each type, totalling 9 activity instances per activity. The final score of a solution is the mean fraction obtain over all 9 instances per activity for all 100 activities. Additionally, we will have a second ranking based on the top 5 activities performed by a solution (mean of all 9 instances in those activities).
            <hr>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Simulation Environment and Datasets</h2>
            
            For this first edition of BEHAVIOR, we provide a fully functional implementation in iGibson 2.0, a new version of our opensource simulation environment. iGibson 2.0 implements all necessary functionalities for the challenge such as object states (temperature, wetness level, cleanliness level, etc.) and sampling functionalities to facilitate development. We also provide the necessary datasets for the Challenge:
            <ul>
                <li><b>iGibson 2.0 Dataset of Scenes:</b> New versions of the fully interactive scenes, more densely populated with objects.</li> 
                <li><b>BEHAVIOR Object Dataset:</b> Dataset of object models annotated with physical and semantic properties. The 3D models are free to use within iGibson 2.0 for BEHAVIOR (due to artists' copyright, models are encrypted and allowed only to be used with iGibson2.0). You can download a bundle of the iGibson2.0 dataset of scenes and the BEHAVIOR dataset of objects <a href="https://storage.googleapis.com/gibson_scenes/behavior_data_bundle.zip">here</a>. </li>
                <li><b><a href="human_dataset">the BEHAVIOR Dataset of Human Demonstrations</a>:</b> To facilitate the development of solutions, we provide a dataset of human successfull executions of the activities in iGibson 2.0 using a virtual reality interface. Humans control the BEHAVIOR Robot embodiment. The dataset includes all state-action pairs and can be deterministically replayed.</li>
            </ul>
            <hr>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Getting Started</h2>
            Do you want to participate? Register in our <a href="https://eval.ai/web/challenges/challenge-page/1190">EvalAI site</a>
             and start downloading and installing the required infrastructure: <a href="http://svl.stanford.edu/igibson/docs/installation.html">a new version of iGibson</a>, our simulation environment for interactive tasks, extended now to new object states for BEHAVIOR, the BEHAVIOR Dataset of Objects and the  iGibson2.0 Dataset of Scenes (combined in our <a href="https://storage.googleapis.com/gibson_scenes/behavior_data_bundle.zip">participation bundle</a>), with object and house models to participate in the challenge, and <a href="https://github.com/StanfordVL/BehaviorChallenge2021/">our starter code</a>, with examplest to train againts in the tasks. If you want to use human demonstrations to start developing your solutions, you can also download <a href="human_dataset">the BEHAVIOR Dataset of Human Demonstrations</a> in virtual reality.
            <hr>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Phases</h2>
            <ul>
              <li><b>Minival Phase:</b>
                The purpose of this phase to make sure your policy can be successfully submitted and
                evaluated. Participants are expected to download our <a
                  href="https://github.com/StanfordVL/BehaviorChallenge2021">starter code</a> and submit a baseline
                policy, even a trivial one, to our evaluation server to verify their entire pipeline is correct.
              <li><b>Dev Phase:</b>
                In this phase, participants will develop their solutions. Solutions can be submitted and to be evaluated on the
                dataset <b>dev</b> split and the leaderboard will be updated within 24 hours. We will provide some quota for submissions per participant.
              </li>
              <li><b> Evaluation Phase:</b> This is the final stage of the challenge and the one that will decide the ranking. Participants are expected to submit a maximum of 5 solutions during the last 15 days of the
                challenge. The solutions will be evaluated on the dataset <b>test</b> split and the results will NOT be
                made available until the end of the challenge.
              </li>
            </ul>
            <hr>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">

            <style>
              table {
                border-collapse: collapse;
                width: 100%;
              }

              th,
              td {
                text-align: left;
                padding: 6px;
              }

              tr:nth-child(odd) {
                background-color: #f2f2f2;
              }
            </style>

            <h2>Timeline</h2>
            <table style="width:50%">
              <tr>
                <td>Challenge Launched</td>
                <td>July 17, 2021</td>
              </tr>
              <tr>
                <td>EvalAI Leaderboard Open, Dev Phase Starts</td>
                <td>August 15, 2021</td>
              </tr>
              <tr>
                <td>Challenge Evaluation Phase Starts</td>
                <td>September 15, 2021</td>
              </tr>

              <tr>
                <td>Challenge Evaluation Phase Ends</td>
                <td>October 10, 2021</td>
              </tr>
              <tr>
                <td>Winner Demo</td>
                <td>October 17, 2021</td>
              </tr>
            </table>
            <hr>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Workshop Speakers</h2><a id="workshop_speakers"></a>
            The results of the first BEHAVIOR Challenge will be announced during our ICCV21 workshop, where we will also enjoy the presentations from the following group of world experts in embodied AI and robotics:</a>
            <div class="row seven-cols">
              <div class="col-md-1">
                <a href="https://www.cs.utexas.edu/users/grauman/" align="middle">
                  <img src="https://www.cs.utexas.edu/sites/default/files/styles/1200_wide/public/news/grauman1-thumbnail.jpg?itok=4Ox5I46j" class="img-circle img-responsive">
                  <font size="3">Kristen Grauman</font>
                </a>
              </div>
              <div class="col-md-1"><a href="https://web.stanford.edu/~bohg/" align="middle">
                  <img src="https://web.stanford.edu/~bohg/img/portrait_square.png" class="img-circle img-responsive">
                  <font size="3">Jeannette Bohg</font>
                </a></div>
              <div class="col-md-1"><a href="https://ai.stanford.edu/~cbfinn/" align="middle">
                  <img src="https://ai.stanford.edu/~cbfinn/_files/ChelseaFinn_hires.jpg" class="img-circle img-responsive">
                  <font size="3">Chelsea Finn</font>
                </a></div>
              <div class="col-md-1"><a href="https://ckllab.stanford.edu/c-karen-liu" align="middle">
                  <img src="https://ckllab.stanford.edu/sites/g/files/sbiybj15216/f/styles/large-scaled/public/karen_headshot_2019_jpeg.png?itok=mB01xRJB" class="img-circle img-responsive">
                  <font size="3">Karen Liu</font>
                </a></div>
              <div class="col-md-1"><a href="https://dcl.wustl.edu/people/jzacks/" align="middle">
                  <img src="https://source.wustl.edu/wp-content/uploads/2015/12/Zacks-Jeffrey.jpg" class="img-circle img-responsive">
                  <font size="3">Jeff Zacks</font>
                </a></div>
              <div class="col-md-1"><a href="https://web.stanford.edu/~hyo/Home.html/" align="middle">
                  <img src="https://psychology.stanford.edu/sites/psychology/files/styles/hs_medium_square_360x360/public/media/capx/gweon-square1582179036530.jpg?h=b4e301e9&itok=CpQI0I3u" class="img-circle img-responsive">
                  <font size="3">Hyowon Gweon</font>
                </a></div>
              <div class="col-md-1"><a href="https://www.csail.mit.edu/person/leslie-kaelbling" align="middle">
                  <img src="https://baicsworkshop.github.io/images/leslie.jpg" class="img-circle img-responsive">
                  <font size="3">Leslie Kaelbling</font>
                </a></div>
            </div>
          </div>
          <hr>
        </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">

        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Organizers and Contact</h2>
            The iGibson Challenge 2021 is organized by the <a href="http://svl.stanford.edu">Stanford Vision and
              Learning Lab</a>. For any inquires, contact us at <a href="mailto:behavior.benchmark@gmail.com">behavior.benchmark@gmail.com</a>
              <p></p>
            <div class="row">
              <div class="col-xs-1 authornames">
                <a href="http://chengshuli.me" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/chengshu.jpg" class="img-circle img-responsive">
                  <font size="3">Chengshu Li</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://www.linkedin.com/in/sanjana-srivastava5/" align="middle">
                  <img src="http://svl.stanford.edu/igibson/images/sanjana.jpg" class="img-circle img-responsive">
                  <font size="3">Sanjana Srivastava</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://github.com/mjlbach/" align="middle">
                  <img src="https://pbs.twimg.com/profile_images/1291846610414997505/J5QTYSTN_400x400.jpg" class="img-circle img-responsive">
                  <font size="3">Michael Lingelbach</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="http://fxia.me" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/fei_xia.png" class="img-circle img-responsive">
                  <font size="3">Fei Xia</font>
                </a>
              </div>

              <div class="col-xs-1 authornames">
                <a href="https://robertomartinmartin.com/" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/roberto_martinmartin.jpg"
                    class="img-circle img-responsive">
                  <font size="3">Roberto Martín-Martín</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://www.cemgokmen.com/" align="middle">
                  <img src="https://www.cemgokmen.com/assets/img/bio-photo.jpg" class="img-circle img-responsive">
                  <font size="3">Cem Gokmen</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://cs.stanford.edu/~shyamal" align="middle">
                  <img src="http://vision.stanford.edu/img/shyamal.jpg" class="img-circle img-responsive">
                  <font size="3">Shyamal Buch</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://ckllab.stanford.edu/c-karen-liu" align="middle">
                  <img src="https://ckllab.stanford.edu/sites/g/files/sbiybj15216/f/styles/large-scaled/public/karen_headshot_2019_jpeg.png?itok=mB01xRJB" class="img-circle img-responsive">
                  <font size="3">Karen Liu</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="http://cvgl.stanford.edu/silvio/" align="middle">
                  <img src="http://pair.stanford.edu/cavin/photos/silvio_savarese.jpg"
                    class="img-circle img-responsive">
                  <font size="3">Silvio Savarese</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://web.stanford.edu/~hyo/Home.html" align="middle">
                  <img src="https://bingschool.stanford.edu/sites/bingschool/files/styles/hs_medium_square_360x360/public/media/capx/gweon1582179036530.jpg?h=1641edec&itok=a3TcTu8Y" class="img-circle img-responsive">
                  <font size="3">Hyowon Gweon</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://jiajunwu.com/" align="middle">
                  <img src="https://jiajunwu.com/images/Jiajun_Wu.jpg" class="img-circle img-responsive">
                  <font size="3">Jiajun Wu</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://engineering.stanford.edu/people/fei-fei-li" align="middle">
                  <img src="https://engineering.stanford.edu/sites/default/files/styles/large-square/public/9818d36da9c8e04dc45603a690b1e7ce.jpg?itok=s_I8jyVm" class="img-circle img-responsive">
                  <font size="3">Li Fei-Fei</font>
                </a>
              </div>
            </div>
          </div>
        </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">

        <div class="section-header">
          <div class="col-lg-12 col-md-12">

            <h2>References</h2>
            <hr>
      <p style="margin-left: 40px">[1] <a target="blank" href="https://arxiv.org/abs/2012.02924">iGibson 1.0: A Simulation Environment 
      for Interactive Tasks in Large Realistic Scenes.</a>. Bokui Shen*, Fei Xia*, Chengshu Li*, Roberto Martín-Martín*, 
      Linxi Fan, Guanzhi Wang, Shyamal Buch, Claudia D'Arpino, Sanjana Srivastava, Lyne P Tchapmi, Micael E Tchapmi, 
      Kent Vainio, Li Fei-Fei, Silvio Savarese, IROS 2021.</p>  
      <p style="margin-left: 40px">[3] <a target="blank" href="https://ieeexplore.ieee.org/abstract/document/8954627/">Interactive
          Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments</a>. Fei Xia, William B.
        Shen, Chengshu Li, Priya Kasimbeg, Micael Tchapmi, Alexander Toshev, Roberto Martín-Martín, and Silvio Savarese.
        RA-L, ICRA 2020.</p>
          </div>
        </div>
      </div>
  </section>
  <footer id="footer" class="midnight-blue">
    <div class="container">
      <div class="row">
        <div class="col-md-6 col-md-offset-3">
          <div class="text-center">
            <a href="#home" class="scrollup"><i class="fa fa-angle-up fa-3x"></i></a>
          </div>

          <div class="credits">
            <!--
              All the links in the footer should remain intact.
              You can delete the links only if you purchased the pro version.
              Licensing information: https://bootstrapmade.com/license/
              Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=OnePage
            -->
            Template designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <!--/#footer-->

  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="js/jquery.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="js/bootstrap.min.js"></script>
  <script src="js/jquery.prettyPhoto.js"></script>
  <script src="js/jquery.isotope.min.js"></script>
  <script src="js/wow.min.js"></script>
  <script src="js/jquery.easing.min.js"></script>
  <script src="js/main.js"></script>

  <script>
    $('#carousel-slider2').carousel({
      interval: false
    })


  </script>

</body>

</html>
