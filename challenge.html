<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <title>iGibson Challenge 2021</title>

  <!-- Bootstrap -->
  <link href="css/bootstrap.min.css" rel="stylesheet">
  <link href="css/animate.min.css" rel="stylesheet">
  <link href="css/animate.css" rel="stylesheet" />
  <link href="css/prettyPhoto.css" rel="stylesheet">
  <link href="css/style.css" rel="stylesheet">
  <!-- =======================================================
    Theme Name: OnePage
    Theme URL: https://bootstrapmade.com/onepage-multipurpose-bootstrap-template/
    Author: BootstrapMade
    Author URL: https://bootstrapmade.com
  ======================================================= -->

  <script src="https://kit.fontawesome.com/39b62c948b.js" crossorigin="anonymous"></script>

</head>
<style>
  .alignright {
    float: right;
  }
</style>

<body>
  <nav class="navbar navbar-default">
    <div class="container">
      <div class="row">
        <div class="site-logo2">
          <img height="45px" src="http://svl.stanford.edu//assets/images/c88c46b9e14bf897e57e597109c97005.png"> </img>

          <img height="45px" src="images/googlelogo.png"> </img>
          <a class="brand">BEHAVIOR Challenge @ ICCV 2021</a>
          <div>
            <a class="brand">
              <h5 class="pull-left">Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments</h5>
              <h5 class="pull-right"> hosted by the Stanford Vision and Learning Lab</h5>
            </a>
          </div>
        </div>


        <!-- /.Navbar-collapse -->
      </div>
    </div>
  </nav>

  <section id="home">
  </section>
  <section id="about">
  </section>


  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
			 <h4> <a href="https://github.com/StanfordVL/iGibsonChallenge2021">[Participation Guidelines]</a> <a href="https://eval.ai/web/challenges/challenge-page/808/overview">[Registration]</a>  <a href="http://svl.stanford.edu/igibson/challenge2020.html">[Starter Code]</a> </h4>
            <p class="section-description">
              A primary goal of embodied AI research is to develop intelligent agents that can assist humans in their everyday lives in activities like washing dishes or cleaning floors. While recent years we have seen excellent new benchmarks that helped the field, they are usually restricted to a few tasks, short horizon or in simplified scenarios. Solving real-world challenges requires a new benchmark with realistic, diverse, and complex activities. We present BEHAVIOR, a benchmark with the 100 household activities that represent a new challenge for embodied AI solutions.
            </p>

            <p class="section-description">
              BEHAVIOR is a challenge where embodied agents make continuous full-body control decisions based on sensor information. Agents need to navigate and manipulate the environment with the goal of acomplishing 100 household activities. BEHAVIOR tests the ability to perceive the environment, plan, and execute complex long-horizon activities that involve multiple objects, rooms, and state transitions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Characteristics of BEHAVIOR</h2>
            <hr>
            <div class="row">
              <div class="col-lg-4 col-md-12">
                <div class="box2 intro">
                  <h4 class="title"><b>100 Common Household Activities</b></h4>
                  <img src="media/beha.gif" width=210 height=210> </img>
                </div>
              </div>
              <div class="col-lg-4 col-md-12">
                <div class="box2 intro">
                  <h4 class="title"><b>Full Control based on Visual Input</b></h4>
                  <img src="media/tpv.png" width=210 height=210> </img>
                </div>
              </div>

              <div class="col-lg-4 col-md-12">
                <div class="box2 intro">
                  <h4 class="title"><b>Involving Multiple State Changes</b></h4>
                  <img src="media/sliced.gif" width=210 height=210> </img>

                </div>
              </div>
            </div>

          </div>
        </div>
      </div>
<!--       <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
           The activities in included in BEHAVIOR are:
            <br>
            <ul>
              <li>Realistic: Preserve high fidelity to the real in the activities, the models, the sensing, and actuation of the agent.</li>
              <li>Diverse: Present variation of scenes, objects, and types of activities.</li>
              <li>Complex: with large time length, number of objects, required skills, and state changes.</li>
            </ul>
            <br>
          </div>
        </div>
      </div> -->
      <p>
      </p>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">

        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Setup</h2>
            <hr>
            <ul>
              <li><b>Embodiment:</b> We accept solutions using any of the two following available embodiments:
              <ul>
                <li><b>BEHAVIOR Robot:</b> Humanoid avatar with two hands, a head and a torso.</li> 
                <li><b>Fetch Robot:</b> Model of the commercially available robot Fetch with a single arm.</li>
              </ul>
              </li>
              Both embodiments generate the same observations but differ on the actuation.
              <li><b>Observations:</b> Participants can choose between two participation tracks that differ by the observations available to the agent. 
              <ul>
                <li><b>Track 1 - On-board Observations:</b> The agent only gets visual signals at 30 fps including RGB, depth, semantic segmentation, and segmentation of activity-relevant objects </li> 
                <li><b>Track 2 - Full observability:</b> The agent has access to privileged information about the state of the environment such as the object poses and their state.</li>
              </ul>
              </li>
              <li><b>Actions:</b> Delta motion for each degree of freedom. For the BEHAVIOR Robot, the agent controls the motion of each hand (6 degrees of freedom) and the closing (1 degree of freedom), the head motion (6 degrees of freedome), and the torso motion (6 degrees of freedome). Head and hands are constrained to certain region around the torso. For the Fetch Robot, the agent controls the delta of each joint of the arm (7 degrees of freedom) and the closing (1 degree of freedom), the head (2 degrees of freedom) and the motion of the base (3 degrees of freedom).</li>
              <li><b>Reward:</b> We provide feedback about the completion of the task. Our tasks are defined by logic conditions and we provide reward for each component of the goal condition that turns TRUE. Feel free
                to create your own.</li>
              <li><b>Termination conditions:</b> The lenght of the episodes are variable depending on the activity and the scene, providing at least 3 times the length required for a human to acomplish it. We also terminate the episode if all elements of the goal condition become TRUE indicating success on the activity.</li>
            </ul>
            <p>The tech spec for the robots and the camera sensor can be found in our <a
                href="https://github.com/StanfordVL/BehaviorChallenge2021/blob/master/Parameters.md">starter code</a>.
            </p>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Evaluation, Metrics and Scoring</h2>
            <hr>
            We evaluate solutions based on the fraction of the elements in the goal condition that are turned TRUE. A solution is evaluated in all 100 activities, in three different types of instances: a) similar to training (only changing location of task relevant objects), b) with different object instances but in the same scenes as in training, and c) in new scenes not seen during training. We use three instances of each type, totalling 9 activity instances per activity. The final score of a solution is the mean fraction obtain over all 9 instances per activity for all 100 activities. Additionally, we will have a second ranking based on the top 5 activities performed by a solution (mean of all 9 instances in those activities).
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Simulation Environment and Datasets</h2>
            <hr>
            For this first edition of BEHAVIOR, we provide a fully functional implementation in iGibson 2.0, a new version of our opensource simulation environment. iGibson 2.0 implements all necessary functionalities for the challenge such as object states (temperature, wetness level, cleanliness level, etc.) and sampling functionalities to facilitate development. We also provide the necessary datasets for the Challenge:
            <ul>
                <li><b>iGibson 2.0 Dataset of Scenes:</b> New versions of the fully interactive scenes, more densely populated with objects.</li> 
                <li><b>BEHAVIOR Object Dataset:</b> Dataset of object models annotated with physical and semantic properties. The 3D models are free to use within iGibson 2.0 for BEHAVIOR.</li>
                <li><b>BEHAVIOR Dataset of Human Demonstrations:</b> To facilitate the development of solutions, we provide a dataset of human successfull executions of the activities in iGibson 2.0 using a virtual reality interface. Humans control the BEHAVIOR Robot embodiment. The dataset includes all state-action pairs and can be deterministically replayed.</li>
              </ul>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">

        <div class="section-header">
          <div class="col-lg-12 col-md-12">
            <h2>Getting Started</h2>
            <hr>
            Do you want to participate? Register in our EvalAI site and start downloading and installing the required infrastructure: <b>a new version of iGibson</b>, our simulation environment for interactive tasks, extended now to new object states for BEHAVIOR, <b>the BEHAVIOR Dataset of Objects</b>, with models to participate in the challenge, and <b>our starter code</b>, with examplest to train againts in the tasks.
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">

            <h2>Phases</h2>
            <hr>
            <ul>
              <li><b>Minival Phase:</b>
                The purpose of this phase to make sure your policy can be successfully submitted and
                evaluated. Participants are expected to download our <a
                  href="https://github.com/StanfordVL/BehaviorChallenge2021">starter code</a> and submit a baseline
                policy, even a trivial one, to our evaluation server to verify their entire pipeline is correct.
              <li><b>Dev Phase:</b>
                In this phase, participants will develop their solutions. Solutions can be submitted and to be evaluated on the
                dataset <b>dev</b> split and the leaderboard will be updated within 24 hours. We will provide some quota for submissions per participant.
              </li>
              <li><b> Evaluation Phase:</b> This is the final stage of the challenge and the one that will decide the ranking. Participants are expected to submit a maximum of 5 solutions during the last 15 days of the
                challenge. The solutions will be evaluated on the dataset <b>test</b> split and the results will NOT be
                made available until the end of the challenge.
              </li>
            </ul>
          </div>
        </div>
      </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">
        <div class="section-header">
          <div class="col-lg-12 col-md-12">

            <style>
              table {
                border-collapse: collapse;
                width: 100%;
              }

              th,
              td {
                text-align: left;
                padding: 6px;
              }

              tr:nth-child(odd) {
                background-color: #f2f2f2;
              }
            </style>

            <h2>Timeline</h2>
            <hr>
            <table style="width:50%">
              <tr>
                <td>Challenge Launched</td>
                <td>July 17, 2021</td>
              </tr>
              <tr>
                <td>EvalAI Leaderboard Open, Dev Phase Starts</td>
                <td>August 15, 2021</td>
              </tr>
              <tr>
                <td>Challenge Evaluation Phase Starts</td>
                <td>September 15, 2021</td>
              </tr>

              <tr>
                <td>Challenge Evaluation Phase Ends</td>
                <td>October 10, 2021</td>
              </tr>
              <tr>
                <td>Winner Demo</td>
                <td>October 17, 2021</td>
              </tr>
            </table>

          </div>
        </div>
      </div>
  </section>
<!--   <section id="method">
    <div class="container">
      <div class="row method-container">

        <div class="section-header">
          <div class="col-lg-12 col-md-12">

            <h2>Participate</h2>
            <hr>
            Do you want to participate? Download our <a
              href="https://github.com/StanfordVL/iGibsonChallenge2021">starter package</a> with iGibson and and start
            developing your own solution right away! You also need to register on the <a href="https://eval.ai/web/challenges/challenge-page/808/overview">EvalAI portal</a>.
          </div>
        </div>
      </div>
  </section> -->
<!--   <section id="method">
    <div class="container">
      <div class="row method-container">

        <div class="section-header">
          <div class="col-lg-12 col-md-12">

            <h2>FAQ</h2>
            <hr>
            <ul>
              <li>
                <b>Q: Where should I watch out for any latest announcement about this challenge?</b>
                <br>
                A: Please check out the EvalAI forum of <a href="https://eval.ai/web/challenges/challenge-page/808/overview">our challenge</a>. We will make all of our
                announcements there.
              </li>
              <li>
                <b>Q: Should I submit the same policy for both tasks? Or should I submit different policies for each of
                  them?</b>
                <br>
                A: You should submit your policy for the two tasks separately on <a href="https://eval.ai/web/challenges/challenge-page/808/overview">EvalAI portal</a>. They are in
                two concurrent phases: Interactive Navigation and Social Navigation. In other words, you should submit
                twice. However, feel free to use the same policy and checkpoint for both tasks if you want.
              </li>
              <li>
                <b>Q: Where can I find the tech spec for the real robot and sensor?</b>
                <br>
                A: You can find the tech spec for the robot <a
                  href="https://www.trossenrobotics.com/locobot-pyrobot-ros-rover.aspx">here</a> and for the sensor
                Intel® RealSense? D435 <a href="https://www.intelrealsense.com/depth-camera-d435/">here</a>. We have
                also summarized these information into a easy-to-read table in our <a
                  href="https://github.com/StanfordVL/iGibsonChallenge2021/blob/master/Parameters.md">starter code</a>.
              </li>
              <li>
                <b>Q: What is the robot's action space?</b>
                <br>
                A: We believe that the best action space for natural and efficient navigation is continuous robot
                velocities (linear and angular). This will be our default action space. However, some participants from
                our last year's challenge has successfully trained policies using discrete action space (e.g. forward,
                turn left, turn right) and then implemented a simple converter that converts the discrete actions to
                robot velocities.
              </li>
              <li>
                <b>Q: What if I have questions that are not answered in FAQ?</b>
                <br>
                A: You are highly encouraged to post your question on the EvalAI forum of <a href="https://eval.ai/web/challenges/challenge-page/808/overview">our challenge</a>.
                Our answers to your questions can help others as well. If you absolutely prefer to ask questions
                privately, you can email us at chengshu@stanford.edu and jaewooj@stanford.edu.
              </li>
              <ul>
          </div>
        </div>
      </div>
  </section> -->
  <section id="method">
    <div class="container">
      <div class="row method-container">

        <div class="section-header">
          <div class="col-lg-12 col-md-12">

            <h2>Organizers</h2>
            <hr>
            The iGibson Challenge 2021 is organized by the <a href="http://svl.stanford.edu">Stanford Vision and
              Learning Lab</a>
            <hr>
            <div class="row">
              <div class="col-xs-1 authornames">
                <a href="http://chengshuli.me" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/chengshu.jpg" class="img-circle img-responsive">
                  <font size="3">Chengshu Li</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://www.linkedin.com/in/jaewoo-jang-a6909968" align="middle">
                  <img src="http://svl.stanford.edu/igibson/images/sanjana.jpg" class="img-circle img-responsive">
                  <font size="3">Sanjana Srivastava</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://www.linkedin.com/in/jaewoo-jang-a6909968" align="middle">
                  <img src="https://pbs.twimg.com/profile_images/1291846610414997505/J5QTYSTN_400x400.jpg" class="img-circle img-responsive">
                  <font size="3">Michael Lingelbach</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="http://fxia.me" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/fei_xia.png" class="img-circle img-responsive">
                  <font size="3">Fei Xia</font>
                </a>
              </div>

              <div class="col-xs-1 authornames">
                <a href="https://robertomartinmartin.com/" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/roberto_martinmartin.jpg"
                    class="img-circle img-responsive">
                  <font size="3">Roberto Martín-Martín</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://ai.google/research/people/AlexanderToshev/" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/chengshu.jpg" class="img-circle img-responsive">
                  <font size="3">Cem Gokmen</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://research.google/people/author35466/" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/chengshu.jpg" class="img-circle img-responsive">
                  <font size="3">Shyamal Buch</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="https://research.google/people/author35466/" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/chengshu.jpg" class="img-circle img-responsive">
                  <font size="3">Karen Liu</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="http://cvgl.stanford.edu/silvio/" align="middle">
                  <img src="http://pair.stanford.edu/cavin/photos/silvio_savarese.jpg"
                    class="img-circle img-responsive">
                  <font size="3">Silvio Savarese</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="http://cvgl.stanford.edu/silvio/" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/chengshu.jpg" class="img-circle img-responsive">
                  <font size="3">Hyowen Gweon</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="http://cvgl.stanford.edu/silvio/" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/chengshu.jpg" class="img-circle img-responsive">
                  <font size="3">Jiajun Wu</font>
                </a>
              </div>
              <div class="col-xs-1 authornames">
                <a href="http://cvgl.stanford.edu/silvio/" align="middle">
                  <img src="http://svl.stanford.edu/assets/img/square/chengshu.jpg" class="img-circle img-responsive">
                  <font size="3">Fei-Fei Li</font>
                </a>
              </div>
            </div>
          </div>
        </div>
  </section>
  <section id="method">
    <div class="container">
      <div class="row method-container">

        <div class="section-header">
          <div class="col-lg-12 col-md-12">

            <h2>References</h2>
            <hr>
      <p style="margin-left: 40px">[1] <a target="blank" href="https://arxiv.org/abs/2012.02924">iGibson, a Simulation Environment 
      for Interactive Tasks in Large Realistic Scenes.</a>. Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, 
      Linxi Fan, Guanzhi Wang, Shyamal Buch, Claudia D'Arpino, Sanjana Srivastava, Lyne P Tchapmi, Micael E Tchapmi, 
      Kent Vainio, Li Fei-Fei, Silvio Savarese, 2020.</p>  
      <p style="margin-left: 40px">[2] <a target="blank" href="https://arxiv.org/abs/1807.06757">On evaluation of embodied navigation
          agents</a>. Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen
        Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, Amir R. Zamir. arXiv:1807.06757, 2018.
      </p>
      <p style="margin-left: 40px">[3] <a target="blank" href="https://ieeexplore.ieee.org/abstract/document/8954627/">Interactive
          Gibson Benchmark: A Benchmark for Interactive Navigation in Cluttered Environments</a>. Fei Xia, William B.
        Shen, Chengshu Li, Priya Kasimbeg, Micael Tchapmi, Alexander Toshev, Roberto Martín-Martín, and Silvio Savarese.
        RA-L, to be presented at ICRA 2020.</p>
      <p style="margin-left: 40px">[4] <a target="blank" href="https://gamma.cs.unc.edu/RVO2/">RVO2 Library: Reciprocal Collision
          Avoidance for Real-Time Multi-Agent Simulation</a>. Jur van den Berg, Stephen J. Guy, Jamie Snape, Ming C.
        Lin, and Dinesh Manocha, 2011.</p>
      <p style="margin-left: 40px">[5] <a target="blank" href="https://arxiv.org/abs/2010.08600">Robot Navigation in Constrained 
      Pedestrian Environments using Reinforcement Learning.</a>. Claudia Pérez-D'Arpino, Can Liu, Patrick Goebel, 
      Roberto Martín-Martín and Silvio Savarese, 2020.</p>
          </div>
        </div>
      </div>
  </section>
  <footer id="footer" class="midnight-blue">
    <div class="container">
      <div class="row">
        <div class="col-md-6 col-md-offset-3">
          <div class="text-center">
            <a href="#home" class="scrollup"><i class="fa fa-angle-up fa-3x"></i></a>
          </div>

          <div class="credits">
            <!--
              All the links in the footer should remain intact.
              You can delete the links only if you purchased the pro version.
              Licensing information: https://bootstrapmade.com/license/
              Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=OnePage
            -->
            Template designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <!--/#footer-->

  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="js/jquery.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="js/bootstrap.min.js"></script>
  <script src="js/jquery.prettyPhoto.js"></script>
  <script src="js/jquery.isotope.min.js"></script>
  <script src="js/wow.min.js"></script>
  <script src="js/jquery.easing.min.js"></script>
  <script src="js/main.js"></script>

  <script>
    $('#carousel-slider2').carousel({
      interval: false
    })


  </script>

</body>

</html>
